kvm与xen的区别：
1.  xen是作为一个单独的用户程序运行的。宿主机只是运行在xen-vmm上的一个特殊操作系统. 而kvm只是linux下的一个内核模块。它和宿主机是一体的。
个人理解：
xen的抽象程度其实是非常高的，但是他需要实现硬件管理，调度的功能。以至于xen的实现复杂，而且很难得到linux社区的支持，而xen则是理想主义。
kvm的抽象程度不如xen，但是他却可以利用linux操作系统的绝大部分功能,如硬件管理，驱动，调度，等等。 kvm是实用主义。

kvm是一个动态可加载的内核模块，运行在host上，分为内核无关模块和内核相关模块. kvm以及kvm－intel。
kvm内核模块在动态加载时，会打开cpu的虚拟化功能，将客户机置于虚拟化模式下运行。

qemu作为一个硬件模拟器，提供硬件模拟，如磁盘，显示器等等，通过ioctl和kvm进行通信。

/dev/kvm这个文件，它是kvm内核模块提供给用户空间的qemu-kvm程序使用的一个控制接口。
比如创建虚拟化接口，一般kvm就会为此虚拟机创建响应的内存数据结构。

虚拟化一般需要分为三个主要部分：cpu虚拟化，内存虚拟化，和io虚拟化。
cpu虚拟化，可以分为指令翻译（vmware主要采用这种技术），和陷入再处理。指令翻译技术性能比较差。而陷入再处理技术，会因为部分敏感指令无法被中断捕获而难以执行。intel的vt－x技术主要是为了解决某些中断指令无法被捕获。通过增加vm－entry和wm－exit事件，再vm－entry时，系统运行在虚拟机下，所有敏感指令此时都可以被捕获。
内存虚拟化技术，刚开始一般通过影子页表解决，再intel的支持下，通过ept，扩展二级页表，可以很简单的实现guest－虚拟内存地址到host－物理内存地址的转换。
io虚拟化，刚开始一般通过模拟器解决。模拟器解决的主要缺陷是性能底下，同时不支持dma映射。intel的vt－d技术，可以捕获dma指令，将guest物理地址转换成host的物理地址。
vpid是描述虚拟机process－id的一个寄存器，通过这个寄存器可以区分不同vm，极大的方便了hyperviser的管理。



而kvm中的一个客户机是作为一个用户空间进程qemu-kvm运行的
kvm可以为每个虚拟机虚拟出一系列的vcpu组，同时可以为vcpu绑定线程亲和性，这样子linux可以以更高效率的方式进行调度。
kvm可以为系统预分配内存，以提高虚拟机的访问速度。
kvm允许内存过载使用，常用的方法有三种：
1.  swap将过载内存交换到磁盘上。效率最低. 
2.  ballooning，再客户机和宿主机的内存中引入气球的概念，如果想让客户机减少内存使用，则宿主机放气给客户机。客户机会尽量回收内存，膨胀气球，以达到此目的。气球内的内存在客户机是不能被使用的。 vsrx并没有使用此项技术。因为vsrx是设备，需要固定的内存以保证系统的正常运行。在云里，这项技术能被广泛使用，以达到内存过载使用。
3.  页共享（内核将多个vm之间相同的页合并)? 这是怎么做到的？


kvm网络可以设置成如下四种：
1.  bridge.
2.  nat
3.  qemu内置的用户网络模式，即直接通过qemu模拟一个网卡，和宿主机共用一个网络ip。如果客户机需要外网的访问，则直接通过port映射来实现，如8080端口映射到客户机的80端口。
4.  vt-d sr-iov

kvm是必须要有硬件虚拟化辅助技术的支持的。而vmware采用二进制翻译技术，则可以无需硬件辅助虚拟化技术的支持。

virtio/vhost 是一种类虚拟化技术。分为
1.    前端(virtio): 负责为客户机的io设备提供接口.需要在客户机中安装。
2.    virtio－ring：负责缓存接口调用信息，以实现批处理。
3.    后端(vhost): 负责调用qemu，实现真正的硬件虚拟化。需要在主机中安装。
virtio－balloning没有被vsrx采用，而virtio－net被re广泛使用。
vsrx是将tso和gro都关闭的，也就是说，不允许硬件对分片进行分片。

virtio-net是对网络进行半虚拟化。
virtio-blk是对磁盘进行半虚拟化。

sr-iov其实就是设备自己虚拟出多个虚拟设备，每个设备拥有独立的寄存器，pci总线地址，寻址空间等等。
优势：
  在宿主机和虚拟机中看到的是多个vf，从而不需要vmm的参与直接实现io虚拟化，提高性能。
劣势：
  需要pci设备的支持。
  不支持动态迁移。(可以利用libvirt实现动态迁移）
  




时间虚拟化在kvm中是很难的一部分，比如时间中断。kvm想要保持准确的时间中断很难，因为真正的硬件终端都是需要被宿主机截获的，而此时宿主机很有可能正在执行其他高优先级的任务，导致时间不准确。

kvm目前对热插拔的支持还不完善，主要支持：cpu，pci的热插拔，也可以通过balloning方式实现内存的热插拔。

虚拟化环境中的迁移分为：
静态迁移：将vm关闭，将镜像文件拷贝到新的vmm上，并重启vm。静态迁移一般无需保存整个镜像文件，可以用数据库类似的snapshot，只保存增量信息。
动态迁移：将在运行过程中的虚拟机迁移到另一台设备中，这项技术在云中有非常广泛的运用，可以实时对虚拟机从一个设备转移到另一个物理设备，从而实现灾难备份，负载均衡，节能，云之间的远程迁移等。 动态迁移不仅包括vmm内部的动态迁移，也包括不同vmm之间的动态迁移，如从kvm迁移到xen等设备。动态迁移也会有短暂的服务暂停时间，一般取决于镜像文件的大小，目的网络带宽等。服务暂停时间一般从几毫米到几秒不等。动态迁移中，如果源宿主机和目的宿主机共享存储设备，则可以只迁移cpu信息等，而不需要迁移整个镜像。动态迁移分如下几步：
  1.  将源vm的cpu内存拷贝到目的机。
  2.  将源vm的cpu信息，和首次拷贝之后的增删改传入目的机。暂停源vm。
  3.  启动目的vm快照。
如果源客户机内存被频繁的修改，而网络速度低于内存修改的速度，则很有可能动态迁移无法完成。
动态迁移和启动客户机的命令行类似，
  目的端：后面增加了一个－incoming tcp:0:<port-num>, 表示监听这个端口，通过tcp传输相应信息。
  源端  : migrate ... 指定目的网络的uri和port。
sr-iov的动态迁移：
  1.  vm启动时用系统默认的非sr－iov网卡。
  2.  迁移时将sriov网卡移除，切换到默认网卡。
  3.  动态迁移。
  4.  目的vm的sriov网卡热插入，并切换目的vm到sriov。
  
nested VM：
  1.  kvm已经支持nested VM。如果L1级上想要支持L2－vm。那么在L0，创建L1是需要用命令-nest=host将L0的硬件初始化环境暴露给L1Vm，同理，多多级Vm也是一样的。
  2.  NestVm 在不同的Vm里支持暂时不是很好。如xen on kvm等等。
  3.  vsrx里的jcp就是nested Vm。nestedVm性能是一个很大的瓶颈。Virtual－box里的Vm想再装一个kvm，速度极其慢。


ethtool是linux下，对网口进行查看和配置的工具。
lspci可以用来查看设备的pci地址
ethtool -i 可以查看driver的信息。
modprobe <driver-name> max_vf=<vf_num> 可以用来加载vf的个数。



linux下挂载新磁盘的命令：
https://zhidao.baidu.com/question/157111059.html

内核模块的加载与卸载：
1.  查看  lsmod | grep tun 
2.  加载  modprobe tun

dhcpclient命令可以用来设置一个端口为dhcp-client: dhclient eth0 

学习kvm虚拟化需要学习的资料
http://www.toutiaofen.com/tag/article/1882344787

